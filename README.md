# AIC-TechLead (Report)


I investigated the integration of attention mechanisms into popular CNN architectures for improved performance. It builds upon foundational research presented in "CBAM: Convolutional Block Attention Module" and "Attention is All You Need."

The core idea lies in enabling the model to selectively focus on crucial features within an image. This approach has shown promise in enhancing tasks like image classification and object detection. The report details the implementation of attention mechanisms within ResNet, MobileNet, and simple CNN architectures. Evaluating these modified networks on benchmark datasets like MNIST, CIFAR-10, and CIFAR-100 will assess the impact on accuracy and potentially reveal architecture-specific benefits.

Furthermore, the research explores the computational efficiency of attention mechanisms, particularly for resource-constrained models like MobileNet. This analysis will determine the trade-off between accuracy improvement and computational cost. Future work will delve deeper into the impact of different attention mechanisms on various architectures and explore their effectiveness in a wider range of applications.



