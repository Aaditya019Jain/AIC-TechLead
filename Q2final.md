Challenges stemming from large datasets in Retrieval-Augmented Generation (RAG) models like InstiGPT include computational inefficiencies, heightened risk of hallucinations, data sparsity leading to reduced model effectiveness, overfitting, and model drift over time, necessitating strategies for robust preprocessing, regularization, and dynamic data updating.

To solve this one possible solution that i found out was to train our own LLM based the our data sets. this would give us the following advantages:-

1. **Customization for Domain-specific Knowledge**: By training our own Language Model on our dataset, we can capture domain-specific nuances, vocabulary, and patterns that may not be present in pre-trained models like Gemini.

2. **Reduced Data Sparsity**: Training a Language Model on our dataset allows we to tailor the model's parameters to the specific distribution of our data. This can mitigate issues related to data sparsity by enabling the model to learn from the entirety of our dataset more effectively, including rare or specialized examples that might be overlooked in generic pre-trained models.

3. **Control Over Training Process**: Training our own LLM gives we full control over the training process, including data preprocessing, model architecture, hyperparameter tuning, and training duration. This control enables we to fine-tune the model according to our specific requirements, potentially improving its performance on tasks such as retrieval and generation.

5. **Mitigating Model Drift**: By continuously retraining our LLM on updated versions of our dataset, we can adapt the model to changes in the underlying data distribution over time. This helps mitigate the risk of model drift, ensuring that the model remains relevant and effective in generating accurate and up-to-date content.

However, it's important to note that training a custom Language Model requires significant computational resources, expertise in machine learning, and careful consideration of ethical and legal implications, particularly regarding data privacy and bias. 

Every time we need to train out LLM we will have to train the complete LLM again to mitigate this problem I have sought a technique to use <b>Incremental Training.</b>

1. **Incremental Training**:
   - Implement a system for incremental training, where new data is periodically added to the existing training dataset, and the LM is retrained on the combined dataset.
   - Use techniques like online learning or mini-batch training to efficiently incorporate new data without having to retrain the entire model from scratch.
   - A brief of what incremental training is given [here](https://www.wetube.com/watch?v=FipRjQRaCz8)
   - the accuracy of the model which was trained completely was almost equal to the efficiency of the model that was trained using incremental training.

2. **Transfer Learning**:
   - Leverage transfer learning techniques to adapt the pre-trained LM to new tasks or domains using limited amounts of new data.
   - Fine-tune the LM on a task-specific dataset or use techniques like domain adaptation to transfer knowledge from the pre-trained LM to the new task or domain.
   - 

Along with that to deal with more data we can implement RAG that uses Gymkhana Servers as its external base and only the data that is not on it will be searched by Google using a FallBack mechanism and in this I is intend to add the filtering component that will filter from which source has the information been used.

The LLM will be periodically retrained using the incremental mechanisms and to keep it up to date the data will be continuosly updated on the Gymkhana Servers as and when the LLM will be trained in the data on Gymkhana servers will be erased and new data will start filling the Storage.

<h3>Another Big Step Towards Real time Data updation and to reduce the hallucination of Insti-GPT - Fresh Prompt Technique</h3>

This concept has been taken from the research paper [Fresh LLM](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://arxiv.org/abs/2310.03214&ved=2ahUKEwip7-XygpyFAxUN1DgGHWJOB4EQFnoECBAQAQ&usg=AOvVaw096ajgwZdynU8Cy2WLy7xP)

<b>Fresh prompts</b> refer to input queries or requests that are updated with recent and relevant information before being used as input for a natural language processing (NLP) model or search engine. The term "fresh" implies that the prompts are augmented or enriched with up-to-date data, ensuring that the resulting responses or search results are timely and aligned with current knowledge or events. Fresh prompts are often utilized to enhance the accuracy, relevance, and recency of responses generated by NLP.

1. **Implement FreshPrompt Technique**: Modify our RAG setup to incorporate the FreshPrompt technique. This involves fetching up-to-date and relevant information from our college servers or other sources and appending it to the prompt. Ensure that the information retrieved is recent and aligned with the context of the query.

2. **Order of Evidences**: Adjust our RAG model to organize the retrieved evidences from our college servers based on their relevance and recency. We can utilize timestamps or other metadata to sort the evidences accordingly. This ensures that the model prioritizes the most relevant and recent information when generating responses.

3. **Additional Information**: Enhance our RAG setup to include additional information beyond organic search results. We can fetch answer boxes, related questions, or supplementary data from our college servers to provide a more comprehensive understanding of the topic. Integrate this additional information into the model's knowledge base to improve response accuracy.

Another thing that can be helpful is Number of Retrieved Evidences, but due to lack of time I wasn't able to think about it much.

5. **Number of Retrieved Evidences**: Adjust the configuration of our RAG model to increase the number of retrieved evidences for each query from our college servers. Experiment with different retrieval strategies and parameters to find the optimal balance between the quantity and quality of retrieved information. Monitor the model's performance to ensure that the increased number of evidences contributes to improved accuracy without overwhelming the system.

<H4>Implementation of Fresh Prompt Technique </H4>

Implementing the FreshPrompt Technique involves dynamically updating the input prompt with up-to-date and relevant information before feeding it into your RAG-based GPT model. 

1. **Define Data Sources**: Identify the sources from which you want to fetch fresh information. This could include web scraping tools, APIs, databases, or any other sources that provide up-to-date and relevant data. In your case, you mentioned using your college servers as the external database. Ensure that you have the necessary permissions and access to retrieve data from these sources.

2. **Retrieve Fresh Information**: Develop scripts or modules to fetch fresh information from the identified sources. Depending on the nature of your data sources, you may need to use techniques such as web scraping, API calls, or database queries to retrieve the desired data. This step should be designed to retrieve information that is relevant to the context of the user query and aligns with the topic being discussed.

3. **Format and Preprocess Data**: Once you retrieve the fresh information, format and preprocess it to make it compatible with the input format expected by your RAG-based GPT model. This may involve cleaning the data, removing irrelevant information, and structuring it in a way that can be easily integrated into the input prompt.

4. **Update Input Prompt**: Modify your RAG setup to dynamically update the input prompt with the fresh information retrieved in the previous steps. This could involve appending the fresh data to the original prompt or replacing certain parts of the prompt with the updated information. Ensure that the updated prompt maintains coherence and relevance to the user query.

5. **Integrate into RAG Pipeline**: Integrate the process of updating the input prompt with fresh information into your RAG pipeline. This may require modifying your existing codebase to accommodate the new functionality seamlessly. Ensure that the integration is robust and does not introduce any performance bottlenecks or stability issues.

6. **Test and Validate**: (<b>not researched much about it </b>)

The tools that we can use to achieve this are:

To implement the FreshPrompt Technique, we'll need a combination of tools and technologies to fetch, process, and integrate fresh information into your RAG-based GPT model. Here's a breakdown of the tools you can use and the workflow to accomplish this task:

1. **Web Scraping Tools**:
   - **Beautiful Soup**: Python library for web scraping. It allows you to parse HTML and XML documents and extract the relevant data.
   - **Scrapy**: A powerful web crawling framework for Python that provides a complete toolset for extracting structured data from websites.

2. **APIs**:
   - **Requests**: Python library for making HTTP requests. You can use it to interact with RESTful APIs and fetch data in JSON or XML format.
   - **API Client Libraries**: If the data sources provide APIs, you might use client libraries specific to those APIs for easier integration and interaction.

3. **Text Processing**:
   - **Natural Language Toolkit (NLTK)**: Python library for natural language processing. It provides tools for tokenization, stemming, and other text processing tasks.
   - **Spacy**: Another popular Python library for natural language processing. It offers features like named entity recognition and dependency parsing.

**Workflow**:

1. **Data Source Identification**: Identify the sources from which you want to fetch fresh information. This could include websites, APIs, databases, or any other relevant sources.

2. **Data Retrieval**:
   - Use web scraping tools or APIs to retrieve data from web pages or external services.
   - If fetching data from databases
   - Fetching Data from all the Sources mentioned can lead to a lot of computation and processing, this will lead to delay in response generation.
   - To deal with this I came up with a method to first implement Google Search and find the top relevant sources or websites and then scrape them to make the Fresh Prompts.
   - Disadvantage this would still lead to delay in the response generation but will give them with high accuracies and Reduced hallucination.
   - One more thing that will lead to Reduction in hallucination will be Hard Coding the rejection of information from some sources like Instagram.

3. **Data Processing**:
   - Preprocess the retrieved data to clean and structure it appropriately.
   - Perform text processing tasks like tokenization, stemming, or entity recognition if needed.

4. **Input Prompt Update**:
   - Modify the input prompt to incorporate the fresh information. This could involve appending the retrieved data to the prompt or replacing specific parts of it.

5. **Integration with RAG Model**:
   - Integrate the updated input prompt with your RAG-based GPT model.
   - Ensure that the integration is seamless and the model can effectively utilize the fresh information during inference.

Solution to Problems created using and their potential solutions 
1. **Parallel Processing**:
   - Utilize parallel processing techniques to scrape information from multiple sources simultaneously.
   - This can help reduce the overall time taken for scraping, especially when fetching data from multiple relevant sources.

2. **Token Limit**:
   - This problem has some solution but don't look like perfect solutions,
     Dealing with token limits or rate limits imposed by websites when scraping data is a common challenge. Here are several strategies to handle this issue effectively:

   1. **Implement Rate Limiting**:
      - Respect the rate limits set by the website by implementing a delay between successive requests. This prevents overwhelming the server with too many requests in a short            period.
      - Use libraries or tools that provide built-in rate limiting functionality, or manually implement delays in your scraping scripts.
   
   2. **Optimize Scraping Efficiency**:
      - Make your scraping process more efficient by fetching only the necessary data from each page.
      - Use selective scraping techniques to target specific elements or sections of the webpage that contain the most relevant information.
      - Avoid unnecessary requests or redundant data retrieval.
   
   3. **Use Caching**:
      - Cache previously scraped data to reduce the number of requests sent to the website.
      - Implement a caching mechanism to store fetched data locally and retrieve it when needed instead of making repeated requests to the website.

   4. **Use Headless Browsers**:
      - Consider using headless browsers like Selenium WebDriver to scrape data. Headless browsers simulate the behavior of a real web browser, allowing you to interact with dynamic content and JavaScript-rendered pages.
      - Use browser automation techniques to navigate through pages and scrape data while mimicking human-like behavior, which can help bypass certain rate-limiting mechanisms.

<br>

 <h3>Alternative to Google Search and Performance Optimization:</h3>
 
  1. The alternative of Google search can be Knowledge Graph, domain-specific corpora, or curated datasets. These are basically datasets with a lot less noise they can be created or their pre-defined datsets can also be used like **WikiData**.
  2. The mehtod that I defined earlier that was of Fresh Prompt Technique could cause many problems like token limits, a lot of hallucination as they contain a lot of noise in them. Knowledge graphs basically provide data with almost zero noise and very precise data.
  3. The data in knowledge graphs is basically organised in Nodes and Edges and they usually contain only a limit of words in between them, this helps them reduce noise to a great extent.
  4. To get greater amount of information or context we basically Use multihop models, they get sentences or information in a sequential oder that can be relevant to the promt entered by the user.
  5. This cannot still ensure that all the data that we ask for is available in the knowledge graph and hence we still need to implement Google Search, All we need to do is ask the model to first look in the knowledge graph and then for Google search.
  6. This process can still need to delay in response generation so instead of making it sequential we can parallelize this process such that both the searching happen parallely, only that if we get a repsonse from the knowledge Graph, Google data will be given less waitage.
  7. Caching can also be one method that can help us in generating relevant and quiker response. The response can be stored at the server for a definite amount of time and whenever the same or similar question is asked the same information can be passed on.

<hr>

   <h3>Defining a model pipeline for fine tuning InstiGPT</h3>
  
   1. I need to take an Open Source model which will allow me to fine tune it, the model that i am planning to use BART (large-sized model), fine-tuned on CNN Daily Mail, it has 406million trainable parameters.BART is a transformer encoder-encoder (seq2seq) model with bidirectional (BERT-like) encoder and an autoregressive (GPT) decoder . BART is pre-trained by corrupting text with an arbitrary noising function and learning a model to reconstruct the original text . This particular checkpoint has been fine-tuned on CNN Daily Mail.
   2. The dataset that I will be using is the ccdv/pubmed-summarization, I was planning to have a dataset that was based on our own data but at this point doing this task manually was not possible.
   3. For Fine tuning the model we can used to finetune all of a model’s parameters for each downstream task, but this is becoming exceedingly costly and impractical . Instead, it is more efficient to train a smaller number of prompt parameters or use a reparametrization method like **low-rank adaptation (LoRA)** to reduce the number of trainable parameters .
   4. what lora does is basically it trains only a few parameters instead of training all of them this saves a lot of computation and cost.
   5. For the deployment part we can fine-tune our model and just put it on the backend container of the gymkhana server VM.

<hr>

<h3>Cost Management and Resource Allocation / Compute and Fine-Tuning Requirements:</h3>

1. facebook/bart-large-cnn occupies around 16GB to 32GB space on GPU depending upon single precision or Half precision
2. But at the same time PEFT can significantly reduce the trainable parameters to 0.23% of the original hence space occupied on the GPU will be around 36.8MB to 73.6MB only.
3. While talking about the fine tuning of our LLM I want to select **llama2** which has 7billion parameters.
4. 7billion parameters use around 14GB of GPU.
5. This can be greatly reduced as while finetuning we can use PEFT (Parameter-Efficient Fine-Tuning) that will greatly reduce the computational requirements and will occupy much less space on GPU.
6. For loading the models we can use Quantised models that require lesser space than the original model with very less trade-off with its accuracy.
7. I conclude to use a 12GB Nvidia GPU that will cost around 13,000INR.
8. We can sought to cloud GPU also that will cost us aroud 50,000INR per year for NVIDIA T4 GPU.
   
<hr>

<h3>Plane Of Action:-</h3>
   - April - May --> Recruit junior engineers for AI community + End Sems (SO not so much work) + month wise division of JE on different projects.
   - May - June --> Discussion of all the projects to be taken up + acquiring requireed skills to do them
   - June - July --> ICML conference + Projetct 1 + Project 2
   - Agust - September --> Project 1 + Project 2 + Project 3(Starting+Learning) ------(Less Work due to intern season)
   - September - October --> Project 3 + Project 4 
   - October - November --> Hactober Fest + Project 4 + InterIIT Prepration
   - November - December --> EndSems + Inter IIT preoration
   - December - Jan --> NuerIPS + InterIIT
   - Jan - Feb --> Project 5 + Project 6
   - Feb - March --> Project 5 + Project 6

There will be a meet every week to discuss the Progress of the projects and To discuss future work
The inter IIT prepration will start from October itself hence only one project is taken along with than
Appropriate ampunt of time will be given for endsems and quizzez to prevent junior engineers from lagging in their acads
The projects mentioned above are actually in this priority order:-
   1. InstiGPT with Fresh Prompt Technique/ Incremental Mechanism
   2. Add Audio recognition to InstiGPT by collaborating with CS Prof
   3. Add programming skills in InstiGPT
   4. Number plate detection for vehicles at IIT maingate
   5. Making Image to text model 
   6. Fine tune InstiGPT to add summarization and few other task
   7. No details of ICSR have been out yet hence Couldn't add it. It is also a conference that we will be attending
<br>
Every time a project is taken half of the team will be working on 1st while the other will be working on the 2nd in the next month I along with the engineers working on 1st project will explain the complete workflow and and code to the 2nd team and vice versa.</br>
After explaination the teams will exchange eachothers projects so that all of them should be able to learn both project.(this is not a Hard point it can be dropped or changed as and when required)
